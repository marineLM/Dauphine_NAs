{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine learning with missing values\n",
    "\n",
    "Here we use simulated data to understand the fundamentals of statistical\n",
    "learning with missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy regression problem with missing values (MCAR).\n",
    "\n",
    "We consider a simple regression problem where X (the data) is bivariate\n",
    "gaussian, and y (the prediction target)  is a linear function of the first\n",
    "coordinate, with noise.\n",
    "\n",
    "### The data-generating mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_without_missing_values(n_samples, rng=42):\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0.9], [0.9, 1]]\n",
    "    if not isinstance(rng, np.random.RandomState):\n",
    "        rng = np.random.RandomState(rng)\n",
    "    X = rng.multivariate_normal(mean, cov, size=n_samples)\n",
    "\n",
    "    epsilon = 0.1 * rng.randn(n_samples)\n",
    "    y = X[:, 0] + epsilon\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick plot reveals what the data looks like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # Smaller default figure size\n",
    "\n",
    "plt.figure()\n",
    "X_full, y_full = generate_without_missing_values(1000)\n",
    "plt.scatter(X_full[:, 0], X_full[:, 1], c=y_full)\n",
    "plt.colorbar(label='y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The missing-values mechanism\n",
    "\n",
    "We now consider missing completely at random settings (MCAR): the missingness\n",
    "is completely independent from the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_mcar(n_samples, missing_rate=.5, rng=42):\n",
    "    X, y = generate_without_missing_values(n_samples, rng=rng)\n",
    "    if not isinstance(rng, np.random.RandomState):\n",
    "        rng = np.random.RandomState(rng)\n",
    "\n",
    "    M = rng.binomial(1, missing_rate, (n_samples, 2))\n",
    "    np.putmask(X, M, np.nan)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick plot to look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = generate_mcar(500)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5', label='All data')\n",
    "plt.colorbar(label='y')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the fully-observed data is the same\n",
    "as that of the original data.\n",
    "\n",
    "### Conditional Imputation with the IterativeImputer\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "As the data is MCAR (a fortiori MAR), an imputer can use the\n",
    "conditional dependencies between the observed and the missing values to\n",
    "impute the missing values.\n",
    "\n",
    "We'll use the IterativeImputer, a good imputer, but it needs to be enabled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "iterative_imputer = IterativeImputer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the imputer on the small data used for visualization. For now, we have not defined train and test sets, we only care about visualization. So we will just fit the imputer on `X` and impute `X`. \n",
    "\n",
    "**The imputation is learned by fitting the imputer on the data with\n",
    "missing values:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data are imputed with the transform method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the imputed data as our previous visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',\n",
    "            label='All data', alpha=.5)\n",
    "plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',\n",
    "            label='Imputed')\n",
    "plt.colorbar(label='y')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the imputer did a fairly good job at recovering the\n",
    "conditional expectation.\n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "Given that the relationship between the fully-observed X and y is a\n",
    "linear relationship, it seems natural to use a linear model for\n",
    "prediction. It must be adapted to missing values using imputation.\n",
    "\n",
    "To use it in supervised setting, we will pipeline it with a linear\n",
    "model, using a ridge, which is a good default linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model performance in a cross-validation loop\n",
    "(for better evaluation accuracy, we increase slightly the number of\n",
    "folds to 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational cost**: One drawback of the IterativeImputer to keep in\n",
    "mind is that its computational cost can become prohibitive for large\n",
    "datasets (it has a bad computational scalability).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean imputation with the SimpleImputer\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "We now try a simple imputer, the imputation by the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick visualization reveals a distortion of the distribution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',\n",
    "            label='All data', alpha=.5)\n",
    "plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',\n",
    "            label='Imputed')\n",
    "plt.colorbar(label='y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning\n",
    "\n",
    "Evaluating the prediction pipeline with cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning without imputation\n",
    "\n",
    "HistGradientBoosting models are based on trees, which can be\n",
    "adapted to model directly missing values. Let's compute their cross-validation\n",
    "scores:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: which pipeline predicts well on our small data?\n",
    "\n",
    "Let's plot the scores to see things better (change the names of the lists\n",
    "containing the scores according to what you used above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,\n",
    "             'IterativeImputer + Ridge': scores_iterative_and_ridge,\n",
    "             'HistGradientBoostingRegressor': score_hist_gradient_boosting,\n",
    "    })\n",
    "\n",
    "sns.boxplot(data=scores, orient='h')\n",
    "plt.title('Prediction accuracy\\n linear and small data\\n'\n",
    "          'Missing Completely at Random')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction performance with more samples.\n",
    "\n",
    "Let us compare models in regimes where there is plenty of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = generate_mcar(n_samples=20000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative imputation and linear model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean imputation and linear model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the HistGradientBoostingRegressor, with mean imputation or using its\n",
    "native handling of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,\n",
    "             'IterativeImputer + Ridge': scores_iterative_and_ridge,\n",
    "             'Mean imputation + HGBR': score_mean_and_hist_gradient_boosting,\n",
    "             'HGBR': score_hist_gradient_boosting,\n",
    "    })\n",
    "\n",
    "sns.boxplot(data=scores, orient='h')\n",
    "plt.title('Prediction accuracy\\n linear and large data\\n'\n",
    "          'Missing Completely at Random')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear data-generating mechanism, the quality of imputation matters when\n",
    "a linear model is used for predictions. The mean imputation thus underperform\n",
    "the iterative imputer. A poor imputation quality can nonetheless be\n",
    "compensated by the use of a more powerful predictive model.\n",
    "\n",
    "Even in the case of a linear data-generating mechanism, the\n",
    "optimal prediction on data imputed by a constant\n",
    "is a piecewise affine function with 2^d regions (\n",
    "http://proceedings.mlr.press/v108/morvan20a.html ). The\n",
    "larger the dimensionality (number of features), the more a\n",
    "imperfect imputation is hard to approximate with a simple model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the predictive model is non-linear\n",
    "\n",
    "We now modify a bit the example above to consider the situation where y is a\n",
    "non-linear function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "X, y= generate_mcar(n_samples, missing_rate=.5)\n",
    "y = y**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using a linear model after the imputation does not make sense.\n",
    "We can however compare different imputations with gradient boosting regression\n",
    "trees and see how much imputation matters in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the model is not linear, the quality of the imputation may\n",
    "not matter as much as it does for a linear model. In our case, the imputation\n",
    "is not the most important step of the pipeline, rather\n",
    "**what is important is to use a powerful predictive model**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a predictor for the fully-observed case\n",
    "\n",
    "We use again our setting with a non-linear function of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "\n",
    "X, y = generate_mcar(n_samples, missing_rate=.5)\n",
    "y = y**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have been able to train a predictive model that works on\n",
    "fully-observed data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_full, y_full = generate_without_missing_values(n_samples)\n",
    "y_full = y_full**2\n",
    "full_data_predictor = HistGradientBoostingRegressor()\n",
    "full_data_predictor.fit(X_full, y_full)\n",
    "\n",
    "cross_val_score(full_data_predictor, X_full, y_full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation reveals that the predictor achieves an excellent\n",
    "explained variance; it is a near-perfect predictor on fully observed\n",
    "data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to data with missing values. We use imputation to build a completed\n",
    "data that looks like the fully-observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "iterative_imputer = IterativeImputer()\n",
    "X_imputed = iterative_imputer.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full data predictor can be used on the imputed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.r2_score(y, full_data_predictor.predict(X_imputed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction is not as good as on the full data, but this is\n",
    "expected, as missing values lead to a loss of information. We can\n",
    "compare it to a model trained to predict on data with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = generate_mcar(n_samples, missing_rate=.5)\n",
    "y_train = y_train**2\n",
    "na_predictor = HistGradientBoostingRegressor()\n",
    "na_predictor.fit(X_train, y_train)\n",
    "\n",
    "metrics.r2_score(y, na_predictor.predict(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained on data with missing values works significantly\n",
    "better than the one trained on the fully-observed data.\n",
    "\n",
    "**Only for linear mechanism is the model on full data also optimal for\n",
    "perfectly imputed data**. When the function linking X to y has\n",
    "curvature, this curvature turns uncertainty resulting from missingness\n",
    "into bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with a (semi-) real dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous experiments give useful insights but were limited to Gaussian data,\n",
    "and a dimension of two. We will now use a more realistic dataset, derived from\n",
    "the\n",
    "[wine quality dataset](https://archive.ics.uci.edu/dataset/186/wine+quality).\n",
    "\n",
    "While the covariates of this dataset are not simulated, the missing values are.\n",
    "We provide two versions of the dataset with two different missing data\n",
    "mechanisms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file1 = open(\"./data/semi_simu1\",'rb')\n",
    "semi_simu1 = pickle.load(file1)\n",
    "\n",
    "file2 = open(\"./data/semi_simu2\",'rb')\n",
    "semi_simu2 = pickle.load(file2)\n",
    "\n",
    "X1, y1 = semi_simu1\n",
    "X2, y2 = semi_simu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1.shape, y1.shape)\n",
    "print(X2.shape, y2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from the analysis on the toy datsets above, and refering to\n",
    "the methods presented in the course, try to obtain the best possible\n",
    "predictions in cross-validation for these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
